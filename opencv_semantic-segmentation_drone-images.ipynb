{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import packages","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport kornia\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport os\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets, transforms, models\nfrom torch.nn import functional as F\nimport torchvision.transforms as T\n\nfrom torchvision.io import read_image\nimport torchvision\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.optim import lr_scheduler\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom dataclasses import dataclass\n\nimport kornia as K\nimport random\n\nimport torchvision.models as models\n","metadata":{"execution":{"iopub.status.busy":"2023-01-01T15:16:21.558222Z","iopub.execute_input":"2023-01-01T15:16:21.558785Z","iopub.status.idle":"2023-01-01T15:16:24.684855Z","shell.execute_reply.started":"2023-01-01T15:16:21.558669Z","shell.execute_reply":"2023-01-01T15:16:24.683216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training parameters","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass TrainingConfiguration:\n    epochs_count: int = 100\n    data_path: str = '/kaggle/input/opencv-pytorch-course-segmentation'\n    num_workers: int = 2\n    batch_size: int = 16\n    plot_interval: int = 5\n    device: str = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    lr: float = 10e-4\n    decay_rate: float = 0.1 \n\nTrainConfig = TrainingConfiguration()","metadata":{"execution":{"iopub.status.busy":"2023-01-01T15:16:26.088745Z","iopub.execute_input":"2023-01-01T15:16:26.089660Z","iopub.status.idle":"2023-01-01T15:16:26.101807Z","shell.execute_reply.started":"2023-01-01T15:16:26.089587Z","shell.execute_reply":"2023-01-01T15:16:26.099457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Find mean and std of the trainset","metadata":{}},{"cell_type":"code","source":"# def get_mean_std(num_workers=2):\n    \n#     transform = transforms.Compose([\n#         transforms.ToPILImage(),\n#         transforms.ToTensor()\n#         ])\n    \n#     loader = torch.utils.data.DataLoader(\n#         SemSegDataset(data_path, csv_fname, train_val_test, transforms = transform),\n#         batch_size=8)\n        \n#     batch_mean = torch.zeros(3)\n#     batch_mean_sqrd = torch.zeros(3)\n    \n#     for batch_data, _ in loader:\n#         batch_mean += batch_data.mean(dim=(0, 2, 3)) \n#         batch_mean_sqrd += (batch_data ** 2).mean(dim=(0, 2, 3)) \n    \n#     mean = batch_mean / len(loader)\n    \n#     var = (batch_mean_sqrd / len(loader)) - (mean ** 2)\n        \n#     std = var ** 0.5\n    \n#     return mean, std\n\n# mean,std = get_mean_std()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T05:28:47.832233Z","iopub.execute_input":"2022-10-29T05:28:47.832595Z","iopub.status.idle":"2022-10-29T05:28:47.837919Z","shell.execute_reply.started":"2022-10-29T05:28:47.832564Z","shell.execute_reply":"2022-10-29T05:28:47.836779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Mean and STD of train set\n# t_mean = [0.4516, 0.5142, 0.4693]\n# t_std = [0.1720, 0.1528, 0.1902]\n\n# Mean and STD from transfer learning\nt_mean = [0.485, 0.456, 0.406] \nt_std = [0.229, 0.224, 0.225]","metadata":{"execution":{"iopub.status.busy":"2023-01-01T15:16:29.540594Z","iopub.execute_input":"2023-01-01T15:16:29.541111Z","iopub.status.idle":"2023-01-01T15:16:29.546628Z","shell.execute_reply.started":"2023-01-01T15:16:29.541072Z","shell.execute_reply":"2023-01-01T15:16:29.545686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Class","metadata":{}},{"cell_type":"code","source":"# addtional transforms        \nadd_transforms = T.Compose([\n        T.ToPILImage(),\n        T.ColorJitter(brightness = 0.2, contrast = 0.2, saturation = 0.2),\n        T.ToTensor(),\n        T.Normalize(t_mean, t_std)\n        ])\n\ncommontransforms = T.Compose([\n        T.ToPILImage(),\n        T.CenterCrop((512,512)),\n        T.ToTensor(),\n        T.Normalize(t_mean, t_std)\n        ])\n\ndef augmentation(image, mask, patchsize = 512):\n        \n        # Random Affine\n        affine_params = T.RandomAffine(0).get_params((0, 0), (0, 0), (0.8, 1.5), (0.9, 1.1,0.9, 1.1), \n                                                          img_size = (image.shape[0],image.shape[1]))\n        image = T.functional.affine(image,*affine_params)\n        mask = T.functional.affine(mask,*affine_params)\n        \n        # Random cropping\n        i, j, h, w = T.RandomCrop.get_params(\n            image, output_size = (patchsize, patchsize))\n        image = T.functional.crop(image, i, j, h, w)\n        mask = T.functional.crop(mask, i, j, h, w)\n        \n        # Random horizontal flipping\n        if random.random() > 0.5:\n            image = T.functional.hflip(image)\n            mask = T.functional.hflip(mask)\n\n        # Random vertical flipping\n        if random.random() > 0.5:\n            image = T.functional.vflip(image)\n            mask = T.functional.vflip(mask)\n        \n        return image, mask\n    \nclass SemSegDataset(Dataset):\n    \"\"\" Generic Dataset class for semantic segmentation datasets.\n\n        Arguments:\n            data_path (string): Path to the dataset folder.\n            images_folder (string): Name of the folder containing the images (related to the data_path).\n            masks_folder (string): Name of the folder containing the masks (related to the data_path).\n            csv_path (string): train or test csv file name\n            image_ids (list): List of images.\n            train_val_test (string): 'train', 'val' or 'test'\n            transforms (callable, optional): A function/transform that inputs a sample\n                and returns its transformed version.\n            class_names (list, optional): Names of the classes.\n            \n\n        Dataset folder structure:\n            Folder containing the dataset should look like:\n            - data_path\n            -- images_folder\n            -- masks_folder\n\n            Names of images in the images_folder and masks_folder should be the same for same samples.\n    \"\"\"\n    def __init__(self, data_path, train_val_test, img_list, transform = None, class_names = None):\n        self.img_fld = data_path + '/imgs/imgs/'\n        self.train_val_test = train_val_test\n        if self.train_val_test != 'test':\n            self.mask_fld = data_path + '/masks/masks/'\n        self.image_ids = img_list\n        if transform is None:\n            self.transforms = T.Compose([\n                T.ToPILImage(),\n                T.ToTensor(),\n                T.Normalize(t_mean, t_std)\n                ])\n        else:\n            self.transforms = transform\n        self.num_class = 12\n    \n    def __len__(self):\n        return len(self.image_ids)\n    \n    def __getitem__(self, idx):\n        img_path = self.img_fld + self.image_ids[idx] + '.jpg'\n        img = read_image(img_path)\n        img = self.transforms(img)\n        \n        if self.train_val_test != 'test':\n            mask_path = self.mask_fld + self.image_ids[idx] + '.png'\n            mask = read_image(mask_path)\n            \n            if self.train_val_test == 'train':\n                img,mask = augmentation(img,mask)\n            else:\n                mask = T.CenterCrop((512,512))(mask)\n        \n        if self.train_val_test == 'test':\n            return img\n        else:\n            return img, mask\n\n#split train validation\nimg_df = pd.read_csv(TrainConfig.data_path +'/'+ 'train.csv').astype(str)\nidx = img_df.sample(frac=1).index.values\ntrain_list = img_df['ImageID'][idx[:int(0.7*len(img_df))]].reset_index().drop('index',axis = 1)\ntrain_list = train_list['ImageID'].to_numpy()\nval_list = img_df['ImageID'][idx[int(0.7*len(img_df)):]].reset_index().drop('index',axis = 1) \nval_list = val_list['ImageID'].to_numpy()\ntest_list = pd.read_csv(TrainConfig.data_path +'/'+ 'test.csv').astype(str)\ntest_list = test_list['ImageID'].to_numpy()\n\n# train dataloader\ntrain_loader = torch.utils.data.DataLoader(\n        SemSegDataset(TrainConfig.data_path, train_val_test = 'train', transform = add_transforms, img_list = train_list),\n        batch_size=TrainConfig.batch_size,\n        shuffle=True,\n        num_workers=TrainConfig.num_workers\n    )\n\n# validation data loader\nval_loader = torch.utils.data.DataLoader(\n        SemSegDataset(TrainConfig.data_path, train_val_test = 'val', img_list = val_list, transform = commontransforms),\n        batch_size = TrainConfig.batch_size,\n        shuffle = False,\n        num_workers = 0\n    )\n\n# test dataloader\ntest_loader = torch.utils.data.DataLoader(\n        SemSegDataset(TrainConfig.data_path, train_val_test = 'test', img_list = test_list),\n        batch_size = TrainConfig.batch_size,\n        shuffle = False,\n        num_workers=0\n    )\n","metadata":{"execution":{"iopub.status.busy":"2023-01-01T15:16:37.645123Z","iopub.execute_input":"2023-01-01T15:16:37.645637Z","iopub.status.idle":"2023-01-01T15:16:37.690969Z","shell.execute_reply.started":"2023-01-01T15:16:37.645571Z","shell.execute_reply":"2023-01-01T15:16:37.689546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization of image and its corresponding mask","metadata":{}},{"cell_type":"code","source":"# sampling one batch to show\nimg_batch = iter(train_loader).next()\nplt.figure(figsize = [16,16])\nfor i,batch in enumerate(img_batch):\n    for j, img in enumerate(batch):\n        plt.subplot(batch.shape[0],2,batch.shape[0]*i+j+1)\n        out_np = img.permute(1,2,0)\n        plt.imshow(out_np.cpu().numpy())\n        plt.axis('off')\nplt.show()\n\nprint(img.shape)","metadata":{"execution":{"iopub.status.busy":"2023-01-01T15:16:49.125784Z","iopub.execute_input":"2023-01-01T15:16:49.126236Z","iopub.status.idle":"2023-01-01T15:16:56.155022Z","shell.execute_reply.started":"2023-01-01T15:16:49.126196Z","shell.execute_reply":"2023-01-01T15:16:56.153950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation metric","metadata":{}},{"cell_type":"code","source":"# import dice loss from kornia\neval_metric = K.losses.DiceLoss()","metadata":{"execution":{"iopub.status.busy":"2023-01-01T15:18:30.267431Z","iopub.execute_input":"2023-01-01T15:18:30.268482Z","iopub.status.idle":"2023-01-01T15:18:30.275149Z","shell.execute_reply.started":"2023-01-01T15:18:30.268429Z","shell.execute_reply":"2023-01-01T15:18:30.273698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# # define frequent used block\n# class DecoderBlock(nn.Module):\n#     def __init__(self, channels_in, channels_out):\n#         super().__init__()\n\n#         self.decoder = nn.Sequential(\n#             nn.Conv2d(channels_in, channels_in // 2, kernel_size = 1, bias = False),\n#             nn.BatchNorm2d(channels_in // 2),\n#             nn.ReLU(),\n#             # Deconvolution\n#             nn.ConvTranspose2d(\n#                 channels_in // 2,\n#                 channels_in // 2,\n#                 kernel_size=2,\n#                 stride=2,\n#                 padding=0,\n#                 output_padding=0,\n#                 groups=channels_in // 2,\n#                 bias=False\n#             ),\n#             nn.BatchNorm2d(channels_in // 2),\n#             nn.ReLU(),\n#             nn.Conv2d(channels_in // 2, channels_out, kernel_size = 1, bias = False),\n#             nn.BatchNorm2d(channels_out),\n#             nn.ReLU()\n#         )\n    \n#     def forward(self, x):\n#         return self.decoder(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # create LinkNet using VGG16 for encoder blocks \n# class LinkNet(nn.Module):\n#     def __init__(self, num_classes):\n#         super().__init__()\n#         vgg16 = getattr(models, 'vgg16')(pretrained=True)\n#         self.dn0 = vgg16.features[0:5]\n#         self.dn1 = vgg16.features[5:10]\n#         self.dn2 = vgg16.features[10:17]\n#         self.dn3 = vgg16.features[17:24]\n#         self.dn4 = vgg16.features[24:31]\n#         self.up1 = DecoderBlock(512,512)\n#         self.up2 = DecoderBlock(512,256)\n#         self.up3 = DecoderBlock(256,128)\n#         self.up4 = DecoderBlock(128,64)\n#         self.lastblock = nn.Sequential(\n#             nn.ConvTranspose2d(64, 32, kernel_size = 3, stride = 2, bias = False),\n#             nn.BatchNorm2d(32),\n#             nn.ReLU(),\n#             nn.Conv2d(32, 32, kernel_size = 3, padding = 0, bias=False),\n#             nn.BatchNorm2d(32),\n#             nn.ReLU(),\n#             nn.ConvTranspose2d(32, num_classes, kernel_size = 2, padding = 0, bias = False)\n#         )\n        \n#     def forward(self, x):\n        \n#         # encoders\n#         en = self.dn0(x)\n#         en1 = self.dn1(en)\n#         en2 = self.dn2(en1)\n#         en3 = self.dn3(en2)\n#         en4 = self.dn4(en3)\n        \n#         # decoders with skip connections (additions)\n#         de1 = self.up1(en4) + en3\n#         de2 = self.up2(de1) + en2\n#         de3 = self.up3(de2) + en1\n#         de4 = self.up4(de3)\n        \n#         # in the paper there is additional block\n#         out = self.lastblock(de4)\n#         return out\n\n# # # check whether the network output correct dim\n# # test_tensor = torch.zeros(4, 3, 320, 320)\n# # model = LinkNet(12)\n# # pred = model(test_tensor)\n# # print(pred.size())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model: DeepLabV3","metadata":{}},{"cell_type":"code","source":"from torchvision.models.segmentation import deeplabv3_resnet101\n\ntest_tensor = torch.zeros(4, 3, 1024, 1024).to(torch.device('cuda'))\nmodel = deeplabv3_resnet101(pretrained = True)\n\n# modify last layer\nmodel.aux_classifier[4] = nn.Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\nmodel.classifier[4] = nn.Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\nmodel.to(torch.device('cuda')).eval()\n\n# freezing resnet101 backbone\nfor i, child in enumerate(model.children()):\n    for param in child.parameters():\n        param.requires_grad = False # retrain the network\n    break\n\npred = model(test_tensor)['out']\nmodel.train()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-10-31T15:00:36.088063Z","iopub.execute_input":"2022-10-31T15:00:36.088530Z","iopub.status.idle":"2022-10-31T15:02:15.738271Z","shell.execute_reply.started":"2022-10-31T15:00:36.088488Z","shell.execute_reply":"2022-10-31T15:02:15.737100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of total params\npytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'total params: {pytorch_total_params}')","metadata":{"execution":{"iopub.status.busy":"2022-10-31T15:05:13.333771Z","iopub.execute_input":"2022-10-31T15:05:13.334272Z","iopub.status.idle":"2022-10-31T15:05:13.343380Z","shell.execute_reply.started":"2022-10-31T15:05:13.334218Z","shell.execute_reply":"2022-10-31T15:05:13.341872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from torchmetrics import JaccardIndex\nfrom kornia.losses import FocalLoss, DiceLoss\n\nTrainConfig = TrainingConfiguration()\n\n# Optimizer\noptimizer = optim.Adam(model.parameters(), lr = TrainConfig.lr ,betas = (0.5,0.999))\n\n# Scheduler\nlmbda = lambda epoch: 1/(1+TrainConfig.decay_rate * epoch)\nscheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lmbda)\n\n#criterion = nn.CrossEntropyLoss(reduction = 'none')\nkwargs = {\"alpha\": 0.5, \"gamma\": 2.0, \"reduction\": 'none'}\ncriterion2 = FocalLoss(**kwargs)\ncriterion = DiceLoss()\ngamma = 2\n\nIoU = JaccardIndex(num_classes = 12, average = 'none').to(TrainConfig.device)\n\nstoremetric = []\n\nfor epoch in range(TrainConfig.epochs_count):\n    epoch_loss = []\n    batch_iou = []\n    for i, data in enumerate(train_loader):\n        \n        x, y = data\n        x = x.to(TrainConfig.device)\n        y = y.to(TrainConfig.device)\n        \n        # reset parameters gradient to zero\n        optimizer.zero_grad()\n        \n        y_pred = model(x)[\"out\"]\n                \n        # calculate focal loss\n        w1 = 0.5\n        w2 = 1-w1\n        loss = w1*criterion(y_pred, y.squeeze().long()) + w2*criterion2(y_pred, y.squeeze().long()).mean()\n        \n        # calculate gradients\n        loss.backward()\n        \n        # update parameters\n        optimizer.step()\n        epoch_loss.append(loss.item())\n        \n        # evaluate mIoU on train set\n        b = y_pred.softmax(dim = 1).argmax(dim = 1).detach().squeeze()\n        a = y.detach().squeeze()\n        eval_metric = IoU(a,b).cpu().numpy()\n        batch_iou.append(eval_metric)\n        \n    #find IoU for each cat.\n    batch_iou = np.array(batch_iou)\n    masked = np.ma.masked_equal(batch_iou, 0)\n    masked_batch_iou = masked.mean(axis = 0)\n    \n    if ((epoch+1) % TrainConfig.plot_interval) == 0: \n        # inference \n        val_metric = []\n        val_loss_arr = []\n        with torch.no_grad():\n            model.eval()\n            for _, val_data in enumerate(val_loader):\n                x_val, y_val = val_data\n                x_val = x_val.to(TrainConfig.device)\n                y_val = y_val.to(TrainConfig.device)\n                y_pred_val = model(x_val.detach())[\"out\"]\n                val_loss = criterion(y_pred_val, y_val.squeeze().long())\n                val_loss_arr.append(val_loss.item())\n                # evaluate mIoU on train set\n                b_val = y_pred_val.softmax(dim = 1).argmax(dim = 1).detach().squeeze()\n                a_val = y_val.detach().squeeze()\n                val_metric.append(IoU(a_val,b_val).cpu().numpy())\n            \n            val_iou = np.array(val_metric)\n            masked = np.ma.masked_equal(val_iou, 0)\n            masked_val_iou = masked.mean(axis = 0)\n            \n            # show sample inference\n            for j in range(y_val.shape[0]):\n                plt.subplot(2,y_val.shape[0],j+1)\n                plt.imshow(y_val[j].cpu().numpy().squeeze(),vmin = 0, vmax = 12)\n                plt.show()\n                plt.subplot(2,y_val.shape[0],j+2)\n                plt.imshow(y_pred_val.softmax(dim = 1).argmax(dim = 1)[j].detach().cpu().numpy().squeeze(),vmin = 0, vmax = 12)\n                plt.show()\n            model.train()\n            print(f'val_mIOU {masked_val_iou.mean()}, val_loss {np.mean(val_loss_arr)}')\n            \n    storemetric.append(masked_batch_iou)\n    \n    # scheduler step/ update learning rate\n    if scheduler is not None:\n        scheduler.step()\n          \n    print(f'epoch {epoch}, loss {np.mean(epoch_loss)}, train_mIoU {storemetric[-1].mean()}')","metadata":{"execution":{"iopub.status.busy":"2022-10-29T05:40:45.956910Z","iopub.execute_input":"2022-10-29T05:40:45.957327Z","iopub.status.idle":"2022-10-29T05:46:00.034113Z","shell.execute_reply.started":"2022-10-29T05:40:45.957296Z","shell.execute_reply":"2022-10-29T05:46:00.032783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create function for converting mask to encoded pixels for submission","metadata":{}},{"cell_type":"code","source":"# Run Length Encoding (RLE)\ndef mask2RLE(img, num_class, filename):\n    flat_img = img.squeeze().flatten()\n    rle_arr = []\n    for ic in range(num_class):\n        mask = []\n        mask = flat_img == ic\n        diff_mask = np.where(mask[:-1] != mask[1:])[0]+1\n        if diff_mask[1::2].shape < diff_mask[::2].shape:\n            start = diff_mask[:-1:2]\n        else:\n            start = diff_mask[::2]\n        lth = diff_mask[1::2] - start\n        EP = np.array((start,lth)).T.reshape(-1).tolist() \n        \n        # if encoded pixel values are empty add NaN\n        if EP == []:\n            msg = float(\"NaN\")\n        else:\n            msg = ' '.join(str(ep) for ep in EP)\n        rle_arr.append(msg)\n\n    name_id = [filename] * num_class\n    name_id = [f'{filename}_{i}' for i in range(num_class)]    \n    DF_encoded = pd.DataFrame(data = {'ImageID' : name_id , 'EncodedPixels' : rle_arr})\n    return DF_encoded\n","metadata":{"execution":{"iopub.status.busy":"2022-10-26T16:16:34.029403Z","iopub.execute_input":"2022-10-26T16:16:34.029800Z","iopub.status.idle":"2022-10-26T16:16:34.041130Z","shell.execute_reply.started":"2022-10-26T16:16:34.029754Z","shell.execute_reply":"2022-10-26T16:16:34.040009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare submission CSV","metadata":{}},{"cell_type":"code","source":"DF = pd.DataFrame()\nwith torch.no_grad():\n    model.eval()\n    for nbatch, test_data in enumerate(test_loader):\n        x_test = test_data.to(TrainConfig.device)\n        y_pred_test = model(x_test.detach())[\"out\"]\n        y_pred_test = y_pred_test.softmax(dim = 1).argmax(dim = 1).detach().squeeze()\n        for ny, y_pred in enumerate(y_pred_test):\n            df = mask2RLE(y_pred.cpu().numpy(), num_class = 12, filename = test_list[(nbatch)*16+ny])\n            DF = pd.concat([DF,df])\n    model.train()\n\nDF.reset_index(drop=True).to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-10-26T16:30:13.880849Z","iopub.execute_input":"2022-10-26T16:30:13.881876Z","iopub.status.idle":"2022-10-26T16:36:29.623355Z","shell.execute_reply.started":"2022-10-26T16:30:13.881835Z","shell.execute_reply":"2022-10-26T16:36:29.622291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Kaggle profile link: https://www.kaggle.com/chayakorn","metadata":{}}]}